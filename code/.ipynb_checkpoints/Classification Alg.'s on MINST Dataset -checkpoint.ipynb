{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data\n",
    "Fetching MINST dataset and plotting the first image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Dataset downloader\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# Working with datastructures such as matrix\n",
    "import numpy as np\n",
    "# Plot in python\n",
    "import matplotlib.pyplot as plt\n",
    "# Dirac delta for misclassification rate\n",
    "from sympy.functions.special.delta_functions import DiracDelta\n",
    "# plot the random forest\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import check_call\n",
    "import pydot\n",
    "import pydot_ng\n",
    "from subprocess import check_call\n",
    "# from tsne import bh_sne\n",
    "\n",
    "# fetching MNIST dataset with fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"/Public/Drop_Box/Career/Research_Projects/DNN/Practical_Machine_Learning/\\\n",
    "data/MNIST/\")\n",
    "\n",
    "# reshape images to 28 * 28 in order to plot\n",
    "images = [x.reshape(28,28) for x in mnist.data]\n",
    "labels = mnist.target\n",
    "\n",
    "# X_2d = bh_sne(images)\n",
    "# rcParams['figure.figsize'] = 20, 20\n",
    "# scatter(X_2d[:, 0], X_2d[:, 1], c=labels)\n",
    "\n",
    "# plotting the first image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0], cmap=plt.cm.gray_r)\n",
    "plt.title('Training: %i' % labels[0])\n",
    "plt.show()\n",
    "plt.close()\n",
    "images = mnist.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The first step in learning is preprocessing. This is step is carry out to\n",
    "- Find and exclude the outliers, \n",
    "- Detect and remove effects of probable errors occured in data collection,\n",
    "- And normalize the data in order to make the algorithms faster and more robust.\n",
    "\n",
    "There are number of procedures in order to preprocess the data each of which are proper for a particular type of data. For MNIST data I prefered to use sklearn preprocessor **MaxAbsScaler** which is appropriate for *sparse* data.[[see this]](http://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "normalized_images = MaxAbsScaler().fit_transform(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning the data\n",
    "In every machine learning project the data shoud be splitted into a training set and a test set. Although there is no optimality about the relative size of these sets I chose 70-30 partition on the data.[[see this]](https://www.researchgate.net/post/Is_there_an_ideal_ratio_between_a_training_set_and_validation_set_Which_trade-off_would_you_suggest) The function **train_test_split** from sklearn.model_selection used to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_images , test_images , train_labels, test_labels = train_test_split(normalized_images, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "In order to plot the classification in 2 dimensions we have to reduce dimensions of the data. I used PCA to this end. PCA from sklearn.decomposition is employed to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e88870a15e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mDR_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDR_Data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "DR_data = pca.fit_transform(images)\n",
    "print(DR_Data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Skip this section!!!! ##todo)\n",
    "# Cross-Validaiton\n",
    "In order to tune the hyperparameteres of a model, a cross-validation should be applied on the data. I prefered to use k-fold (k=5) cross-validation to do the hyperparameter tuning. The function **StratifiedKFold** from sklearn.model_selection has been used in order to avoid different proportions of classes in folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "i = 0\n",
    "for train_index, test_index in skf.split(train_images, train_labels):\n",
    "    train_im[i], val_im[i] = train_images[train_index], train_images[test_index]\n",
    "    train_lab[i], val_lab[i] = train_labels[train_index], train_labels[test_index]\n",
    "    i += 1\n",
    "    \n",
    "def acc_fold(clf):\n",
    "    acc_sum = 0\n",
    "    for i in range(0,5):\n",
    "        fit = clf.fit(train_im[i],train_lab[i])\n",
    "        pred = fit.predict(val_im[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrix\n",
    "In order to evaluate the classification I used confusion matrix. The code below generates a plot of the input confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cnf(cnf_matrix, cnf_title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "    plt.title(cnf_title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, [str(x) for x in range(0,10)], rotation=45)\n",
    "    plt.yticks(tick_marks, [str(x) for x in range(0,10)])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()  \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized linear models\n",
    "Fitting a linear model on data by multiplication of a weight vector to a measurement matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression\n",
    "Logistic regression is the simplest form of classification algorithms. It fits a linear model on data and put it in a euqation with a logit function which in some sense the probability of the data to belong to a specific class (it is a rough explanation for more details [see this](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)).<br />\n",
    "Logistic regression is implemented as **LogisticRegression** function in sklearn.linear_model. The function has a parameter to set the type of penalty one may want to induce to the weights. I applied the logisitic regression in 3 fashions on the data: \n",
    "- one with no regularization term, \n",
    "- one with l1-norm normalizaiton \n",
    "- and one with l2 norm normalizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "# NO REGULARIZATION\n",
    "# define the classifier\n",
    "# regularization coefficient is set to 0\n",
    "# solver is set to saga due to documentation advice on large number of samples\n",
    "# multiclass is set to multinomial because of multiclasses\n",
    "LR_clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=0, fit_intercept=True,\\\n",
    "                   solver='saga',multi_class='multinomial',n_jobs=1) \n",
    "# fitting the model to training data\n",
    "LR_fit = LR_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "LR_pred = LR_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, LR_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"logistic regression with no regularization\")\n",
    "print(\"accuracy rate of logistic regression with no regularization: %d\", accuracy_score(test_labels, ridge_pred))\n",
    "\n",
    "# L1 REGULARIZATION\n",
    "# define the classifier\n",
    "# regularization coefficient is set to 0\n",
    "# solver is set to saga due to documentation advice on large number of samples\n",
    "# multiclass is set to multinomial because of multiclasses\n",
    "Cs = np.arange(0.5,10,0.5) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "LR1_clf = GridSearchCV(estimator=LogisticRegression(penalty='l1', dual=False, tol=0.0001, fit_intercept=True,\\\n",
    "                   solver='saga',multi_class='multinomial'), param_grid=dict(C=Cs), n_jobs=2)\n",
    "\n",
    "# fitting the model to training data\n",
    "LR1_fit = LR1_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "LR1_pred = LR1_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, LR1_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"logistic regression with l1 regularization\")\n",
    "print(\"accuracy rate of logistic regression with l1 regularization: %d\", accuracy_score(test_labels, LR1_pred))\n",
    "\n",
    "# L2 REGULARIZATION\n",
    "# define the classifier\n",
    "# regularization coefficient is set to 0\n",
    "# solver is set to sag due to documentation advice on large number of samples\n",
    "# multiclass is set to multinomial because of multiclasses\n",
    "Cs = np.arange(0.5,10,0.5) # defining the grid of search space\n",
    "LR2_clf = GridSearchCV(estimator=LogisticRegression(penalty='l1', dual=False, tol=0.0001, fit_intercept=True,\\\n",
    "                   solver='sag',multi_class='multinomial'), param_grid=dict(C=Cs), n_jobs=2) \n",
    "# fitting the model to training data\n",
    "LR2_fit = LR2_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "LR2_pred = LR2_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, LR2_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"logistic regression with l2 regularization\")\n",
    "print(\"accuracy rate of logistic regression with l2 regularization: %d\", accuracy_score(test_labels, LR2_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge classifier\n",
    "Ridge classifier induces an L2 regularization on the coefficients of the linear model. [[see this]](http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf) The method has been implemented using **RidgeClassifier** from sklearn.linear_model.<br />\n",
    "In order to find the best regularization strength (alpha) a cross-validation has been carried out. The function **RidgeClassifierCV** from sklearn.linear_model is used to do a cross-validation. I'd rather using 5-fold cross-validation according to [this](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). However, there are several approches to apply cross-validation on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "# defining the ridge classifier\n",
    "#grid search between 0.5 and 10 with step of size 0.5\n",
    "ridge_clf = RidgeClassifierCV(alphas=np.arange(0.5, 10, 0.5), fit_intercept=True, cv=5) \n",
    "# fitting the model to training data\n",
    "ridge_fit = ridge_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "ridge_pred = ridge_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, ridge_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"ridge classification\")\n",
    "print(\"accuracy rate of ridge classification: %d\", accuracy_score(test_labels, ridge_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 norm classifier\n",
    "There is no implementation of lasso (l1 regulization) classifier in sklearn. Thus, I prefered to use a linear svm calssifier with l1 penalty. To this end, **LinearSVC** has been utilized from sklearn.svm. In order to find the regularizaiton hyperparameter **GridSearchCV** has been used from sklearn.model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score # impelent cross validation and find regularization hyperparameter\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "\n",
    "Cs = np.arange(0.5,10,0.5) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "Linear_SVM_clf = GridSearchCV(estimator=LinearSVC(penalty='l1', loss='squared_hinge', dual=False,\\\n",
    "                      tol=0.0001, multi_class='multinomial', fit_intercept=True), param_grid=dict(C=Cs), n_jobs=2)\n",
    "\n",
    "\n",
    "\n",
    "# fitting the model to training data\n",
    "Linear_SVM_fit = Linear_SVM_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "LS_pred = Linear_SVM_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, LS_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"Lasso classification\")\n",
    "print(\"accuracy rate of Lasso classification: %d\", accuracy_score(test_labels, LS_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net classifier\n",
    "We talked about linear models with l1- and l2-norm regularizaiton but what if we combine them? <br />\n",
    "Elastic net is a linear combination of l1- and l2-norm.[[see this]](https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf) It can somhow benefit both of the methods. Parameteres of **SGDClassifier** form sklearn.linear_model may be set to apply a Elastic net regularization on categorial data to carry out a classification (l1 and l2 are also implemented). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score # impelent cross validation and find regularization hyperparameter\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "\n",
    "alphas = np.arange(0.01,2,0.01) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "# penalty set to elasticnet\n",
    "# ratio of l1 regularization is 0.15 \n",
    "# learning rate is not constant\n",
    "elasticnet_clf = GridSearchCV(estimator=SGDClassifier(loss='hinge', penalty='elastic-net', l1_ratio=0.15, fit_intercept=True,\\\n",
    "              n_jobs=3, learning_rate='optimal'), param_grid=dict(alpha=alphas), n_jobs=2)\n",
    "\n",
    "# fitting the model to training data\n",
    "elasticnet_fit = elasticnet_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "elasticnet_pred = elasticnet_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, elasticnet_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"elasticnet classification\")\n",
    "print(\"accuracy rate of elasticnet classification: %d\", accuracy_score(test_labels, elasticnet_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "Perceptron is the first and most naive implementation of a nural network.[[see this]](https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf) The point is the learning process which has several iterations and updates in the fitted weights.<br />\n",
    "In sickit-learn the perceptron has been developed as **Perceptron** in sklearn.linear_model. Regularization is also available to add on perceptron. All four conditions has been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score # impelent cross validation and find regularization hyperparameter\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "# NO REGULARIZATION\n",
    "prc = Perceptron(fit_intercept=True,n_jobs=2)\n",
    "# fitting the model to training data\n",
    "prc_fit = prc.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "prc_pred = prc_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, prc_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"perceptron\")\n",
    "print(\"accuracy rate of perceptron: %d\", accuracy_score(test_labels, prc_pred))\n",
    "\n",
    "# L1 REGULARIZATION\n",
    "alphas = np.arange(0.01,2,0.01) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "# penalty set to elasticnet\n",
    "# ratio of l1 regularization is 0.15 \n",
    "# learning rate is not constant\n",
    "prc_L1 = GridSearchCV(estimator=Perceptron(penalty = 'l1', fit_intercept=True,n_jobs=2), param_grid=dict(alpha=alphas), n_jobs=2)\n",
    "# fitting the model to training data\n",
    "prc_L1_fit = prc_L1.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "prc_L1_pred = prc_L1_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, prc_L1_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"perceptron with L1\")\n",
    "print(\"accuracy rate of perceptron with L1: %d\", accuracy_score(test_labels, prc_L1_pred))\n",
    "\n",
    "# L2 REGULARIZATION\n",
    "alphas = np.arange(0.01,2,0.01) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "# penalty set to elasticnet\n",
    "# ratio of l1 regularization is 0.15 \n",
    "# learning rate is not constant\n",
    "prc_L2 = GridSearchCV(estimator=Perceptron(penalty = 'l2', fit_intercept=True,n_jobs=2), param_grid=dict(alpha=alphas), n_jobs=2)\n",
    "# fitting the model to training data\n",
    "prc_L2_fit = prc_L2.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "prc_L2_pred = prc_L2_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, prc_L2_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"perceptron with L2\")\n",
    "print(\"accuracy rate of perceptron with L2: %d\", accuracy_score(test_labels, prc_L2_pred))\n",
    "\n",
    "# elasticnet REGULARIZATION\n",
    "alphas = np.arange(0.01,2,0.01) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "# penalty set to elasticnet\n",
    "# ratio of l1 regularization is 0.15 \n",
    "# learning rate is not constant\n",
    "prc_elastic = GridSearchCV(estimator=Perceptron(penalty = 'elasticnet', fit_intercept=True,n_jobs=2), param_grid=dict(alpha=alphas), n_jobs=2)\n",
    "# fitting the model to training data\n",
    "prc_elastic_fit = prc_elastic.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "prc_elastic_pred = prc_elastic_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, prc_elastic_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"perceptron with elasticnet\")\n",
    "print(\"accuracy rate of perceptron with elasticnet: %d\", accuracy_score(test_labels, prc_elastic_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passiveagressive classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score # impelent cross validation and find regularization hyperparameter\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "Cs = np.arange(0.01,2,0.01) # defining the grid of search space\n",
    "# defining the classifier using grid search to set the regularization strength\n",
    "# penalty set to elasticnet\n",
    "# ratio of l1 regularization is 0.15 \n",
    "# learning rate is not constant\n",
    "pac_clf = GridSearchCV(estimator=PassiveAggressiveClassifier(C=1.0, fit_intercept=True, loss='hinge'), param_grid=dict(C=Cs), n_jobs=2)\n",
    "# fitting the model to training data\n",
    "pac_clf_fit = pac_clf.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "pac_clf_pred = pac_clf_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, pac_clf_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"Passiveaggressive classification\")\n",
    "print(\"accuracy rate of Passiveaggressive classification: %d\", accuracy_score(test_labels, pac_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrement analysis\n",
    "Given you have gaussian distributed multivariate data points and you want to classify the data by introducing linear or quadratic boundries in the feature space. LDA and QDA are aimed to find such boundries with a maximum likelihood approach.[[see this]](http://web.stanford.edu/class/stats202/content/lec9.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11db4c208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy rate of LDA:  0.867523809524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# LDA\n",
    "# solver set to lsqr to benefit shrinkage\n",
    "# shrinkage rate is set to auto to do the optimal one\n",
    "# defining the classifier LDA\n",
    "LDA = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "# fitting the model to training data\n",
    "LDA_fit = LDA.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "LDA_pred = LDA_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, LDA_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"LDA\")\n",
    "print(\"accuracy rate of LDA: \", accuracy_score(test_labels, LDA_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# h = .02\n",
    "# x_min, x_max = images[:, 0].min() - .5, images[:, 0].max() + .5\n",
    "# y_min, y_max = images[:, 1].min() - .5, images[:, 1].max() + .5\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "\n",
    "# print(np.c_[xx.ravel(), yy.ravel()])\n",
    "# cm = plt.cm.RdBu\n",
    "# cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# plt_lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "# plt_lda.fit(train_images[:,0:2], train_labels)\n",
    "# score = plt_lda.score(test_images[:,0:2], test_labels)\n",
    "\n",
    "# ax = plt\n",
    "# # Plot the decision boundary. For that, we will assign a color to each\n",
    "# # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# Z = plt_lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "# # Put the result into a color plot\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "# # Plot also the training points\n",
    "# ax.scatter(train_images[:, 0], train_images[:, 1], c=train_labels, cmap=cm_bright,\\\n",
    "#            edgecolors='k')\n",
    "# # and testing points\n",
    "# ax.scatter(test_images[:, 0], test_images[:, 1], c=test_labels, cmap=cm_bright,\\\n",
    "#            edgecolors='k', alpha=0.6)\n",
    "# ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\\\n",
    "#         size=15, horizontalalignment='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11de272e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy rate of QDA: %.2 0.544952380952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score # the accuracy of the prediction\n",
    "from sklearn.metrics import confusion_matrix # a matrix indicating the prediciton\n",
    "\n",
    "# QDA\n",
    "# defining the classifier QDA\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "# fitting the model to training data\n",
    "QDA_fit = QDA.fit(train_images, train_labels)\n",
    "# predicting the labels of test data\n",
    "QDA_pred = QDA_fit.predict(test_images)\n",
    "# defining the confusion matrix of the prediction\n",
    "cnf_matrix = confusion_matrix(test_labels, QDA_pred)\n",
    "# visualizing the confusion matrix\n",
    "plot_cnf(cnf_matrix, \"QDA\")\n",
    "print(\"accuracy rate of QDA: \", accuracy_score(test_labels, QDA_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "RandomForestClassifier function has been used to set the parameteres.\n",
    "- The function \"misclass\" gets the max depth of the trees of the forest and returns the misclassification rate for 10-fold cross-validation.\n",
    "- The funciton opt_param finds the optimal depth of the trees by a grid search. (optimal depth was 190 for a grid serach between 10 and 200 by step of length 10)\n",
    "- The first classifer tree fitted on the training data has been plotted and the plot is saved in tree0.png.\n",
    "- In order to find the best number of classifiers for this problem the out-of-bag error among the number of trees has been plotted. (optimal number of trees was \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fits a random forest to the training data with maximum depth of m_depth and returns \n",
    "# the misclassification rate for 10-fold cross-validation\n",
    "def misclass(m_depth):\n",
    "    # Define Random Forest Classifier by setting the parameters\n",
    "    # n_estimators : number of random trees\n",
    "    # criterion : mesearement of feature importance \n",
    "    # max_depth : maximum depth of trees in the forest\n",
    "    # min_samples_split : minimum number of samples in a split\n",
    "    # min_samples_leaf : minimum number of samples in a leaf\n",
    "    # min_weight_fraction_leaf : minimum weight of samples to be a leaf\n",
    "    # n_jobs : number of cores dedicated\n",
    "    # bootstrap : whether to use bootstrap for sampling or not\n",
    "    # oob_score : whether to calculate out of box score or not\n",
    "    # max_fetures : maximum fetures used for classification at each split\n",
    "    # max_leaf_nodes : maximum number of leaf nodes\n",
    "    # min_impurity_split : if the impurity of a node is more than this the node can be splited.\n",
    "    # min_impurity_decrease : if the impurity of the nodes resulted from spliting a node is more than this the node can be splited.\n",
    "    # warm_start : Whether to use previous model and add new estimators\n",
    "    # random_state : seed of the random generator\n",
    "    # verbose : ?\n",
    "    \n",
    "    # setting the parameters of the classifier\n",
    "    rfclassifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=m_depth, min_samples_split=2, min_samples_leaf=20,\\\n",
    "                           min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\\\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False,\\\n",
    "                           n_jobs=3, random_state=None, verbose=0, warm_start=False)\n",
    "    mis_rate = 0\n",
    "    # the following is a 10-fold cross-validation\n",
    "    for x in range(0,10):\n",
    "        min_ind = x * training_size // 10\n",
    "        max_ind = (x + 1) * training_size // 10\n",
    "        # validation data\n",
    "        vd_img = training_images[min_ind:max_ind]\n",
    "        vd_lab = training_labels[min_ind:]\n",
    "        # training data\n",
    "        tr_img = np.concatenate((training_images[:min_ind] , training_images[max_ind:]), axis = 0)\n",
    "        tr_lab = np.concatenate((training_labels[:min_ind], training_labels[max_ind:]), axis = 0)\n",
    "        # fitting the model to training data\n",
    "        rfclassifier.fit(tr_img, tr_lab)\n",
    "        # predicting the labels of validation data\n",
    "        predicitons = rfclassifier.predict(vd_img)\n",
    "        # computing misclassificaitons of validation data\n",
    "        misclassifications = [(p_i - l_i == 0) for p_i, l_i in zip(predicitons, vd_lab)]\n",
    "        mis_rate += sum(misclassifications) / 3500\n",
    "    return 1- mis_rate / 10\n",
    "\n",
    "# optimizer finder employing grid search between mini and maxi with grid width of step\n",
    "def opt_param(mini, maxi, step):\n",
    "    opt = 1\n",
    "    for x in range(mini, maxi, step):\n",
    "        if opt > misclass(x):\n",
    "            opt = misclass(x)\n",
    "            optx = x\n",
    "    return optx\n",
    "\n",
    "# fits a classifier with n_est trees to the training data\n",
    "def rfclf(n_est):\n",
    "    # A classifier description --> oob_score = True for future use in plotting\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, criterion='gini', max_depth= 190, min_samples_split=2, min_samples_leaf=20,\\\n",
    "                               min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\\\n",
    "                               min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=True,\\\n",
    "                               n_jobs=3, random_state=None, verbose=0, warm_start=True)\n",
    "\n",
    "    # Fitting the defined classifier\n",
    "    clf.fit(training_images, training_labels)\n",
    "    return clf\n",
    "    \n",
    "rf_clf = rfclf(100)\n",
    "\n",
    "# plotting the first classifier tree\n",
    "tree = rf_clf.estimators_[0]\n",
    "export_graphviz(tree,\n",
    "            filled=True,\n",
    "            rounded=True)\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree0.png')\n",
    "\n",
    "# plotting out-of-bag error for different number of trees\n",
    "oob_errors = []\n",
    "for n in range(10, 1210,30):\n",
    "    rf_clf = rfclf(n)\n",
    "    oob_errors.append(1 - rf_clf.oob_score_)   \n",
    "plt.plot(range(10, 1210,30), oob_errors, 'r')\n",
    "plt.show()  \n",
    "\n",
    "# Took about 30 mins on core i5 intel --> best number of trees =? 500 (best performance without overhead)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fits a classifier with n_est trees to the training data\n",
    "def gbclf(n_est):\n",
    "    # Defining a Gradient Boosting Classifier (many of parameters are the same as of RF classifier)\n",
    "    # loss : Gradient Boosting (deviance) or Adaboost (exponential)\n",
    "    # learning rate : rate of decrease in weight of residual fitted tree\n",
    "    # subsample : fraction of data to be used if < 1 --> stochastic gradient descent\n",
    "    # criterion :  “friedman_mse” for the mean squared error with improvement score by Friedman, “mse” for mean squared error, and “mae” for the mean absolute error. best is friedman!\n",
    "    # init : ?\n",
    "    # presort : whether to presort data in order to speedup the alg. best is auto!\n",
    "    clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=n_est,\\\n",
    "                                       subsample=1.0, criterion='friedman_mse',\\\n",
    "                                       min_samples_split=2, min_samples_leaf=1,\\\n",
    "                                       min_weight_fraction_leaf=0.0, max_depth=3,\\\n",
    "                                       min_impurity_decrease=0.0, min_impurity_split=None,\\\n",
    "                                       init=None, random_state=None, max_features=None,\\\n",
    "                                       verbose=0, max_leaf_nodes=None, warm_start=False,\\\n",
    "                                       presort='auto')\n",
    "    # Fitting the model\n",
    "    clf.fit(training_images, training_labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the methods\n",
    "Now we want to compare the methods in terms of misclassifications of test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# misclassification rate of Random Forest Classifier\n",
    "rf_mis_rate = []\n",
    "# misclassification rate of Gradient Boosting Classifier\n",
    "gb_mis_rate = []\n",
    "# between 10 and 1200 trees\n",
    "for n in range(10, 1210,30):\n",
    "    \n",
    "    rf_clf = rfclf(n)\n",
    "    rf_pred = rf_clf.predict(test_images)\n",
    "    rf_mis = [(p_i - l_i == 0) for p_i, l_i in zip(rf_pred, test_labels)]\n",
    "    rf_mis_rate.append(sum(rf_mis) / 3500)\n",
    "    \n",
    "    gb_clf = gbclf(n)\n",
    "    gb_pred = gb_clf.predict(test_images)\n",
    "    gb_mis = [(p_i - l_i == 0) for p_i, l_i in zip(gb_pred, test_labels)]\n",
    "    gb_mis_rate.append(sum(gb_mis) / 3500)\n",
    "    \n",
    "plt.plot(range(10, 1210,30), rf_mis_rate , 'b', range(10, 1210,30), gb_mis_rate, 'r')\n",
    "plt.show()\n",
    "\n",
    "# took more than a hour!!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
