{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data\n",
    "Fetching MINST dataset and plotting the first image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ea3c358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Dataset downloader\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# Working with datastructures such as matrix\n",
    "import numpy as np\n",
    "# Plot in python\n",
    "import matplotlib.pyplot as plt\n",
    "# Dirac delta for misclassification rate\n",
    "from sympy.functions.special.delta_functions import DiracDelta\n",
    "# plot the random forest\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import check_call\n",
    "import pydot\n",
    "import pydot_ng\n",
    "from subprocess import check_call\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fetching MNIST dataset with fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"/Users/elihei/programming/Practical Machine Learning/data/MNIST\")\n",
    "\n",
    "# reshape images to 28 * 28 in order to plot\n",
    "images = [x.reshape(28,28) for x in mnist.data]\n",
    "labels = mnist.target\n",
    "\n",
    "# plotting the first image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0], cmap=plt.cm.gray_r)\n",
    "plt.title('Training: %i' % labels[0])\n",
    "plt.show()\n",
    "plt.close()\n",
    "images = mnist.data\n",
    "\n",
    "# making (label, features) --> each pixel is a feature so we have 784 features for each sample\n",
    "# half of the data is used for training and half will be used for testing.\n",
    "indice = np.random.choice(len(images), len(images)//2, replace=False) # random choice of half of the indices --> to be unbiased!\n",
    "training_images = images[indice]\n",
    "training_labels = labels[indice]\n",
    "test_images = images[[x for x in range(0,len(images)) if x not in indice]]\n",
    "test_labels = labels[[x for x in range(0,len(images)) if x not in indice]]\n",
    "# making classes \n",
    "map(int, training_labels)\n",
    "training_size = len(training_images)\n",
    "\n",
    "# ATTENTION! the data should be in a 2d array format. Each image is a row and\n",
    "# all of the images should compose a 2d array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "RandomForestClassifier function has been used to set the parameteres.\n",
    "- The function \"misclass\" gets the max depth of the trees of the forest and returns the misclassification rate for 10-fold cross-validation.\n",
    "- The funciton opt_param finds the optimal depth of the trees by a grid search. (optimal depth was 190 for a grid serach between 10 and 200 by step of length 10)\n",
    "- The first classifer tree fitted on the training data has been plotted and the plot is saved in tree0.png.\n",
    "- In order to find the best number of classifiers for this problem the out-of-bag error among the number of trees has been plotted. (optimal number of trees was \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/tree/export.py:399: DeprecationWarning: out_file can be set to None starting from 0.18. This will be the default in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGiRJREFUeJzt3X1wVfWdx/H3FyIESZCniGKYggjW\n+NCKgdpWrW13Fa0D0xZasDOKfXC3HTt13Z0Ojq3O0odZd7vVtsu00vrUR6tuW6nQ0ien2q0ikbYo\nIBBRIVQlIAqEp4R894/fucm9NzfJTQg5957zec2cueeee+7N7+TA53zP79zzi7k7IiKSDkPiboCI\niAwehb6ISIoo9EVEUkShLyKSIgp9EZEUUeiLiKSIQl9EJEUU+iIiKaLQFxFJkYq4G5Bv/PjxPnny\n5LibISJSVp555pld7l7T23olF/qTJ0+moaEh7maIiJQVM3u5mPXUvSMikiIKfRGRFFHoi4ikiEJf\nRCRFigp9M5ttZpvMrNHMFhd4/RIzW2tmbWY2r8Dro8ysycz+ZyAaLSIi/dNr6JvZUGApcAVQByw0\ns7q81bYBi4Afd/MxXwIe738zRURkIBRT6c8CGt19q7sfAR4A5mav4O4vufs6oD3/zWZ2ATAB+M0A\ntFdERI5BMaF/GrA963lTtKxXZjYE+G/g33pZ73ozazCzhubm5mI+uqu9e+G22+Dpp/v3fhGRFDje\nF3I/A6x096aeVnL3Ze5e7+71NTW93lBWWGsrLFkCTz3Vv/eLiKRAMXfk7gAmZT2vjZYV453AxWb2\nGaAKGGZm+929y8XgY1ZdHR737x/wjxYRSYpiQn8NMM3MphDCfgFwdTEf7u4fy8yb2SKg/rgEPsCw\nYXDCCbBv33H5eBGRJOi1e8fd24AbgFXARuBBd19vZkvMbA6Amc00syZgPnCXma0/no3uVnW1Kn0R\nkR4UNeCau68EVuYtuzVrfg2h26enz7gPuK/PLeyLqipV+iIiPUjWHbmq9EVEepSs0FelLyLSo2SF\nvip9EZEeJSv0q6oU+iIiPUhe6Kt7R0SkW8kKfXXviIj0KFmhr0pfRKRHyQr96mo4dAja2uJuiYhI\nSUpW6FdVhUd18YiIFJSs0NegayIiPUpW6GcqffXri4gUlKzQV6UvItKjZIW+Kn0RkR4lK/RV6YuI\n9ChZoa9KX0SkR8kKfVX6IiI9Slbo63v6IiI9SlbojxwZHtW9IyJSULJCf8iQEPyq9EVECkpW6IMG\nXRMR6UHyQl/DK4uIdCt5oa9KX0SkW8kLfVX6IiLdSl7oq9IXEelW8kJflb6ISLeSF/qq9EVEupW8\n0FelLyLSreSFfqbSd4+7JSIiJSd5oV9dDe3t4Q+ki4hIjuSFvoZXFhHpVnJDX/36IiJdJC/0Naa+\niEi3khf66t4REelW8kJflb6ISLeSF/qq9EVEupW80FelLyLSreSFvip9EZFuFRX6ZjbbzDaZWaOZ\nLS7w+iVmttbM2sxsXtbyt5vZk2a23szWmdlHB7LxBanSFxHpVq+hb2ZDgaXAFUAdsNDM6vJW2wYs\nAn6ct/wAcI27nw3MBu40s9HH2ugeDRsGFRWq9EVECqgoYp1ZQKO7bwUwsweAucCGzAru/lL0Wnv2\nG919c9b8381sJ1ADvHHMLe+OmQZdExHpRjHdO6cB27OeN0XL+sTMZgHDgBf6+t4+0/DKIiIFDcqF\nXDM7FfgBcJ27txd4/XozazCzhubm5mP/gar0RUQKKib0dwCTsp7XRsuKYmajgBXALe7+VKF13H2Z\nu9e7e31NTU2xH909VfoiIgUVE/prgGlmNsXMhgELgOXFfHi0/s+B77v7w/1vZh9VVanSFxEpoNfQ\nd/c24AZgFbAReNDd15vZEjObA2BmM82sCZgP3GVm66O3fwS4BFhkZn+Nprcfly3JVl2tSl9EpIBi\nvr2Du68EVuYtuzVrfg2h2yf/fT8EfniMbew7VfoiIgUl745c0IVcEZFuJDP0dSFXRKSgZIZ+dTUc\nPAhHj8bdEhGRkpLM0NefTBQRKSiZoa9B10RECkpm6Gt4ZRGRgpIZ+qr0RUQKSmboq9IXESkomaGv\nSl9EpKBkhr4qfRGRgpId+qr0RURyJDP0M907qvRFRHIkM/RHjgyPqvRFRHIkM/SHDoUTT1Toi4jk\nSWbogwZdExEpILmhr+GVRUS6SG7oq9IXEekiuaGvSl9EpIvkhr4qfRGRLpIb+qr0RUS6SG7oq9IX\nEekiuaGvSl9EpIvkhn6m0nePuyUiIiUj2aF/9CgcPhx3S0RESkZyQ1+DromIdJHc0NfwyiIiXSQ3\n9FXpi4h0kdzQV6UvItJFckNffydXRKSL5Ia+/k6uiEgXyQ19VfoiIl0kN/RV6YuIdJHc0FelLyLS\nRXJDf/jw8LdyVemLiHRIbuibadA1EZE8yQ190PDKIiJ5kh/6qvRFRDokO/Srq1Xpi4hkKSr0zWy2\nmW0ys0YzW1zg9UvMbK2ZtZnZvLzXrjWzLdF07UA1vCiq9EVEcvQa+mY2FFgKXAHUAQvNrC5vtW3A\nIuDHee8dC9wGvAOYBdxmZmOOvdlFUqUvIpKjmEp/FtDo7lvd/QjwADA3ewV3f8nd1wHtee+9HPit\nu7/u7nuA3wKzB6DdxVGlLyKSo5jQPw3YnvW8KVpWjGN577FTpS8ikqMkLuSa2fVm1mBmDc3NzQP3\nwar0RURyFBP6O4BJWc9ro2XFKOq97r7M3evdvb6mpqbIjy5CdTUcOBD+Vq6IiBQV+muAaWY2xcyG\nAQuA5UV+/irgMjMbE13AvSxaNjgyg661tAzajxQRKWW9hr67twE3EMJ6I/Cgu683syVmNgfAzGaa\nWRMwH7jLzNZH730d+BLhwLEGWBItGxwadE1EJEdFMSu5+0pgZd6yW7Pm1xC6bgq99x7gnmNoY/9p\neGURkRwlcSH3uNHfyRURyZHs0M9076jSFxEBkh76qvRFRHIkO/RV6YuI5Eh26KvSFxHJkezQV6Uv\nIpIj2aE/cmR4VKUvIgIkPfQrKmDECFX6IiKRZIc+aNA1EZEsyQ/96mqFvohIJPmhX1Wl7h0RkUjy\nQ1+VvohIh+SHvip9EZEO6Qh9VfoiIkAaQl9/J1dEpEPyQ1+VvohIh+SHfqbSd4+7JSIisUt+6FdV\nQVsbHDkSd0tERGKX/NDXoGsiIh2SH/oaXllEpEPyQ1+VvohIh+SHvip9EZEOyQ99VfoiIh2SH/qq\n9EVEOiQ/9DOVvkJfRCQFoZ+p9NW9IyKSotBXpS8ikoLQHzEChgxRpS8iQhpC30yDromIRJIf+qDh\nlUVEIukIfVX6IiJAWkJflb6ICJCW0FelLyICpCX0VemLiABpCX1V+iIiQFpCX5W+iAiQltBXpS8i\nAqQl9KuroaUF2tvjbomISKyKCn0zm21mm8ys0cwWF3h9uJn9NHp9tZlNjpafYGb3m9mzZrbRzG4e\n2OYXKTP+TktLLD9eRKRU9Br6ZjYUWApcAdQBC82sLm+1TwB73P0M4A7g9mj5fGC4u58LXAD8U+aA\nMKg06JqICFBcpT8LaHT3re5+BHgAmJu3zlzg/mj+YeD9ZmaAAyPNrAIYARwB9g5Iy/tCfz1LRAQo\nLvRPA7ZnPW+KlhVcx93bgDeBcYQDQAvwCrAN+Jq7v36Mbe47VfoiIsDxv5A7CzgKTASmAP9qZqfn\nr2Rm15tZg5k1NDc3D3wrVOmLiADFhf4OYFLW89poWcF1oq6ck4DdwNXAr9291d13Av8H1Of/AHdf\n5u717l5fU1PT963ojSp9ERGguNBfA0wzsylmNgxYACzPW2c5cG00Pw/4g7s7oUvnfQBmNhK4EHh+\nIBreJ6r0RUSAIkI/6qO/AVgFbAQedPf1ZrbEzOZEq90NjDOzRuAmIPO1zqVAlZmtJxw87nX3dQO9\nEb1SpS8iAkBFMSu5+0pgZd6yW7PmDxG+npn/vv2Flg86VfoiIkBa7shVpS8iAqQl9CsqoLJSlb6I\npF46Qh806JqICGkKfQ2vLCKSotBXpS8iotAXEUmT9IS+undERFIU+qr0RURSFPqq9EVEUhT6qvRF\nRFIU+qr0RURSFPpVVdDaCkeOxN0SEZHYpCf0NeiaiEiKQl+DromIpCj0VemLiKQo9EePDo8vvxxv\nO0REYpSe0L/4Yjj5ZPj2t+NuiYhIbNIT+pWV8OlPw4oVsHlz3K0REYlFekIfQugPGwbf+EbcLRER\niUW6Qn/CBLj6arjvPtizJ+7WiIgMunSFPsCNN8KBA/C978XdEhGRQZe+0H/b2+C974VvfQva2uJu\njYjIoEpf6EOo9rdvh5/9LO6WiIgMqnSG/lVXwdSpcOedcbdERGRQpTP0hwyBz30OnnwSVq+OuzUi\nIoMmnaEPcN11cNJJqvZFJFXSG/pVVfDJT8JDD4X+fRGRFEhv6APccAO4w9KlcbdERGRQpDv0J0+G\nD30Ili2Dlpa4WyMictylO/QhfH1zzx74/vfjbomIyHGn0H/Xu2DmzDAeT3t73K0RETmuFPpmodrf\ntAl+/eu4WyMiclwp9AHmz4eJE+GrX4W//EV/PF1EEqsi7gaUhBNOgJtvhs9+FmbMCMMvn3suXHBB\neD5jRnheWRl3S0VEjom5e9xtyFFfX+8NDQ3x/PDGRnjmGVi7tvMxMwRzRQW85z3hbGDWrHjaJyLS\nDTN7xt3re1tPlX62M84I00c/Gp67w0svhfBfswbuvRfe8Y7w+le+EsbvEREpI+rT74kZTJkCH/4w\n/Md/hDOBL34RfvlLOOuscAF49+64WykiUjSFfl9UV8OSJbBlCyxaFMbknzoVbr8dDh6Mu3UiIr0q\nKvTNbLaZbTKzRjNbXOD14Wb20+j11WY2Oeu188zsSTNbb2bPmln5Xw2dODHcxbtuHVx8MSxeDNOn\nwx13QFNT3K0TEelWr6FvZkOBpcAVQB2w0Mzq8lb7BLDH3c8A7gBuj95bAfwQ+Gd3Pxu4FGgdsNbH\n7eyzQ1fPY49BbS3cdBNMmgTvfne42WvHjrhbKCKSo5hKfxbQ6O5b3f0I8AAwN2+ducD90fzDwPvN\nzIDLgHXu/jcAd9/t7kcHpukl5NJLw9j8mzbBl78M+/eH/v7aWrjoIh0ARKRkFBP6pwHZYw83RcsK\nruPubcCbwDhgOuBmtsrM1prZ5wv9ADO73swazKyhubm5r9tQOqZPh1tugb/9DZ5/Hr70Jdi3LxwA\nJk0KQzm/8UbcrRSRFDveF3IrgIuAj0WPHzSz9+ev5O7L3L3e3etramqOc5MGyZlnwhe+0HkAuPHG\n8JXPc86BFSvibp2IpFQxob8DmJT1vDZaVnCdqB//JGA34azgcXff5e4HgJXAjGNtdNk580z4+tfh\nqadg9OjwN3qvuQZefz3ulolIyhQT+muAaWY2xcyGAQuA5XnrLAeujebnAX/wcKvvKuBcMzsxOhi8\nB9gwME0vQzNnhjt9v/hF+MlPwoXgX/zi2D+3vR2OJu9SiYgMvF5DP+qjv4EQ4BuBB919vZktMbM5\n0Wp3A+PMrBG4CVgcvXcP8HXCgeOvwFp3T3ffxvDh4bv+a9bAKafABz8ICxfCrl09v+/w4dBNtGIF\nfPOb4Q+7X3VVuElsxAgYNw4+/nH4zW+grW1wtkVEyo7G3olTa2u4sWvJks7gbm0Nod3Wljt/+HDu\ne6uqwo1hU6eGoSNefTWcNezdC+PHw7x5sGBBuI9giO7BE0m6YsfeUeiXgueeCzd2tbaGgd0qKsLI\nn9nzJ54Ip5/eGfQ1NWGYiGyHDoW/CfDAA+H+gQMHwo1kH/lICP+JE8N0yilhJNFC2trCeENbtnRO\n27fDhAlhSIrsafz4rm0YTEePht/d9OnhoCmSYgr9tGtpgUcfDQeAX/2q65nCySd3HgQmTICdO0PA\nb92a2z1UXR2+bvraa13HGaqqCn9nuLY2HETyD1SZxyFDej44nHEGzJkTDiTF2LYN7r4b7rkn3AFd\nWQnvex984ANw5ZWhTd05fDh0rT3+ODzxBLz8cmj78OGdj9nz48Z1PdiNGVNcO48X9/A3HzK/WxEU\n+pJt374Q5jt2wN//3vmYmX/11XDmMG1a5zR9eng8+eTOwN67N5wFvPhimDLzr7wSzlIy3VH5XVQ9\nXWRub++8d+Hcc2Hu3DBdcEHugaK1NZy9fPe7sGpVWHbZZaEb69lnw7WOF14Iy+vqOg8Ab3tbCPkn\nnghBv3p15wGwrg7e+tbO7rMjR3IfDx8OB8P8eytOOimE/+TJ4YCV/fuaOPHYzn4OHoTf/Q4eeSQM\n83HwYDiDO3iwczp0KAT/iBGh/XV14dpO5nHq1HBAKEW7d4cipLISzjsvtHXo0LhblQgKfSkfjY2w\nfHkIuj/9KRwIJk4M1f/ll4e7ne+7LwRwbW24YH3ddbkVvXs4U1mxAlauhD/+MRwoMoYOhfPPh0su\nCV1dF10UuqeK8cYbXQ90L74YDqRbt+aeRZ14Yu7BM/sgWqhLDqC5ObT7kUfChfgDB2DUqDCMd1VV\nCPfKyvCYmSorw8X/jRthw4Zw9pNxwgnhZ44c2fX6UObx6NFwNpP/mdnz2Wdr+V2OY8eGb6NdcEH4\nOT3ZvTtcb3roIfj973PPJEeMCPeunHde7jR2bHH7JuPo0fC7WLMmFDGTJsFb3hKmiRNDmweaezjT\nfO65zmnbtvDz8s8Oa2sLH4jdw1n5vn1hMgv7rh8U+lKedu0KAbh8eajoW1pCYF91FXzqUzB7dnGV\n4b59IWA2bAjhdOGFoatqoB09Gv7jb9kCmzfnXgvJ7yobNSr3QDBiRDhA/fnP4UBXWxsOdHPnhqE9\nurvuUsj+/eHbXRs2hOn558PBqFBgZ7qFjhzJPYPIPqs4dKjwlwqyDxoQPuecc8IBKjOddRa8+WYI\n+gcf7Az6KVPC9aUPfzjsw3Xrws2Lmcfs7sPx4zvPnrLPpM44IxxkGhtDwDc0hMe1a8PBspChQ+G0\n0zoPAiefHD6ju8m96xlfZv7AgbCfMyG/d2/nzzn11PD5r7wSroO1t+e2obY2HPj37+8M+f37c9e7\n8MJQ5PSDQl/K36FD4T/AmWeG6qnctLaGawb5B4TNm8Ny99D9lAn6GTPivTDeFzt3hrBdvTpMTz/d\n2Q1WVRX2XSbo588PYd/T9rmHCn3dutBdl/l9bd4cuiGzjRjROZR5ZWU4g5s5M0z19SFcm5pC1f3y\ny51T5vmuXSG8+5t9Y8eGA132dPbZuWcnra0h+PPPDl9/Pfx+Ro0KRUj2NGpU+Hf+3vf2q1kKfZFS\ndvhwqBKTMuxIe3uovjMHgJEjw/WW/Gsz/dHSEj47c9B87bUQtDNnhusY/bl+4R4OHC0tXachQ7pe\n3M88VlaGazoleHBW6IuIpEixoa/ve4mIpIhCX0QkRRT6IiIpotAXEUkRhb6ISIoo9EVEUkShLyKS\nIgp9EZEUKbmbs8ysGXi5j28bD/Typ6fKirantGl7Sltat+ct7t7rLd4lF/r9YWYNxdyJVi60PaVN\n21PatD09U/eOiEiKKPRFRFIkKaG/LO4GDDBtT2nT9pQ2bU8PEtGnLyIixUlKpS8iIkUo+9A3s9lm\ntsnMGs1scdzt6Y2ZTTKzx8xsg5mtN7PPRcvHmtlvzWxL9DgmWm5m9s1o+9aZ2Yx4t6AwMxtqZn8x\ns0ej51PMbHXU7p+a2bBo+fDoeWP0+uQ4212ImY02s4fN7Hkz22hm7yzn/WNm/xL9W3vOzH5iZpXl\ntn/M7B4z22lmz2Ut6/M+MbNro/W3mNm1cWxL1I5C2/Nf0b+5dWb2czMbnfXazdH2bDKzy7OW9z3/\n3L1sJ2Ao8AJwOjAM+BtQF3e7emnzqcCMaL4a2AzUAf8JLI6WLwZuj+avBH4FGHAhsDrubehmu24C\nfgw8Gj1/EFgQzX8H+HQ0/xngO9H8AuCncbe9wLbcD3wymh8GjC7X/QOcBrwIjMjaL4vKbf8AlwAz\ngOeylvVpnwBjga3R45hofkwJbc9lQEU0f3vW9tRF2TYcmBJl3tD+5l/sO/MYf3HvBFZlPb8ZuDnu\ndvVxGx4B/hHYBJwaLTsV2BTN3wUszFq/Y71SmYBa4PfA+4BHo/9su7L+AXfsJ2AV8M5oviJaz+Le\nhqxtOSkKSctbXpb7Jwr97VHQVUT75/Jy3D/A5LyQ7NM+ARYCd2Utz1kv7u3Je+2DwI+i+Zxcy+yj\n/uZfuXfvZP5BZzRFy8pCdOp8PrAamODur0QvvQpMiObLYRvvBD4PtEfPxwFvuHtb9Dy7zR3bE73+\nZrR+qZgCNAP3Rt1V3zOzkZTp/nH3HcDXgG3AK4Tf9zOU7/7J1td9UtL7Ks/HCWcrMMDbU+6hX7bM\nrAr4X+BGd9+b/ZqHw3ZZfK3KzK4Cdrr7M3G3ZYBUEE67v+3u5wMthK6DDmW2f8YAcwkHs4nASGB2\nrI06Dsppn/TGzG4B2oAfHY/PL/fQ3wFMynpeGy0raWZ2AiHwf+TuP4sWv2Zmp0avnwrsjJaX+ja+\nG5hjZi8BDxC6eL4BjDazimid7DZ3bE/0+knA7sFscC+agCZ3Xx09f5hwECjX/fMPwIvu3uzurcDP\nCPusXPdPtr7uk1LfV5jZIuAq4GPRgQwGeHvKPfTXANOibyIMI1x4Wh5zm3pkZgbcDWx0969nvbQc\nyHyb4FpCX39m+TXRNxIuBN7MOqWNnbvf7O617j6Z8Pv/g7t/DHgMmBetlr89me2cF61fMhWau78K\nbDezM6NF7wc2UKb7h9Ctc6GZnRj928tsT1nunzx93SergMvMbEx0BnRZtKwkmNlsQjfpHHc/kPXS\ncmBB9M2qKcA04Gn6m39xX5wZgIshVxK+AfMCcEvc7SmivRcRTkPXAX+NpisJ/aa/B7YAvwPGRusb\nsDTavmeB+ri3oYdtu5TOb++cHv3DbAQeAoZHyyuj543R66fH3e4C2/F2oCHaR78gfNOjbPcP8O/A\n88BzwA8I3wIpq/0D/IRwTaKVcDb2if7sE0JfeWM0XVdi29NI6KPP5MJ3sta/JdqeTcAVWcv7nH+6\nI1dEJEXKvXtHRET6QKEvIpIiCn0RkRRR6IuIpIhCX0QkRRT6IiIpotAXEUkRhb6ISIr8PxtoRVrh\nsOz3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120eeb160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fits a random forest to the training data with maximum depth of m_depth and returns \n",
    "# the misclassification rate for 10-fold cross-validation\n",
    "def misclass(m_depth):\n",
    "    # Define Random Forest Classifier by setting the parameters\n",
    "    # n_estimators : number of random trees\n",
    "    # criterion : mesearement of feature importance \n",
    "    # max_depth : maximum depth of trees in the forest\n",
    "    # min_samples_split : minimum number of samples in a split\n",
    "    # min_samples_leaf : minimum number of samples in a leaf\n",
    "    # min_weight_fraction_leaf : minimum weight of samples to be a leaf\n",
    "    # n_jobs : number of cores dedicated\n",
    "    # bootstrap : whether to use bootstrap for sampling or not\n",
    "    # oob_score : whether to calculate out of box score or not\n",
    "    # max_fetures : maximum fetures used for classification at each split\n",
    "    # max_leaf_nodes : maximum number of leaf nodes\n",
    "    # min_impurity_split : if the impurity of a node is more than this the node can be splited.\n",
    "    # min_impurity_decrease : if the impurity of the nodes resulted from spliting a node is more than this the node can be splited.\n",
    "    # warm_start : Whether to use previous model and add new estimators\n",
    "    # random_state : seed of the random generator\n",
    "    # verbose : ?\n",
    "    \n",
    "    # setting the parameters of the classifier\n",
    "    rfclassifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=m_depth, min_samples_split=2, min_samples_leaf=20,\\\n",
    "                           min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\\\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False,\\\n",
    "                           n_jobs=3, random_state=None, verbose=0, warm_start=False)\n",
    "    mis_rate = 0\n",
    "    # the following is a 10-fold cross-validation\n",
    "    for x in range(0,10):\n",
    "        min_ind = x * training_size // 10\n",
    "        max_ind = (x + 1) * training_size // 10\n",
    "        # validation data\n",
    "        vd_img = training_images[min_ind:max_ind]\n",
    "        vd_lab = training_labels[min_ind:]\n",
    "        # training data\n",
    "        tr_img = np.concatenate((training_images[:min_ind] , training_images[max_ind:]), axis = 0)\n",
    "        tr_lab = np.concatenate((training_labels[:min_ind], training_labels[max_ind:]), axis = 0)\n",
    "        # fitting the model to training data\n",
    "        rfclassifier.fit(tr_img, tr_lab)\n",
    "        # predicting the labels of validation data\n",
    "        predicitons = rfclassifier.predict(vd_img)\n",
    "        # computing misclassificaitons of validation data\n",
    "        misclassifications = [(p_i - l_i == 0) for p_i, l_i in zip(predicitons, vd_lab)]\n",
    "        mis_rate += sum(misclassifications) / 3500\n",
    "    return 1- mis_rate / 10\n",
    "\n",
    "# optimizer finder employing grid search between mini and maxi with grid width of step\n",
    "def opt_param(mini, maxi, step):\n",
    "    opt = 1\n",
    "    for x in range(mini, maxi, step):\n",
    "        if opt > misclass(x):\n",
    "            opt = misclass(x)\n",
    "            optx = x\n",
    "    return optx\n",
    "\n",
    "# fits a classifier with n_est trees to the training data\n",
    "def rfclf(n_est):\n",
    "    # A classifier description --> oob_score = True for future use in plotting\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, criterion='gini', max_depth= 190, min_samples_split=2, min_samples_leaf=20,\\\n",
    "                               min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\\\n",
    "                               min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=True,\\\n",
    "                               n_jobs=3, random_state=None, verbose=0, warm_start=True)\n",
    "\n",
    "    # Fitting the defined classifier\n",
    "    clf.fit(training_images, training_labels)\n",
    "    return clf\n",
    "    \n",
    "rf_clf = rfclf(100)\n",
    "\n",
    "# plotting the first classifier tree\n",
    "tree = rf_clf.estimators_[0]\n",
    "export_graphviz(tree,\n",
    "            filled=True,\n",
    "            rounded=True)\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree0.png')\n",
    "\n",
    "# plotting out-of-bag error for different number of trees\n",
    "oob_errors = []\n",
    "for n in range(10, 1210,30):\n",
    "    rf_clf = rfclf(n)\n",
    "    oob_errors.append(1 - rf_clf.oob_score_)   \n",
    "plt.plot(range(10, 1210,30), oob_errors, 'r')\n",
    "plt.show()  \n",
    "\n",
    "# Took about 30 mins on core i5 intel --> best number of trees =? 500 (best performance without overhead)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits a classifier with n_est trees to the training data\n",
    "def gbclf(n_est):\n",
    "    # Defining a Gradient Boosting Classifier (many of parameters are the same as of RF classifier)\n",
    "    # loss : Gradient Boosting (deviance) or Adaboost (exponential)\n",
    "    # learning rate : rate of decrease in weight of residual fitted tree\n",
    "    # subsample : fraction of data to be used if < 1 --> stochastic gradient descent\n",
    "    # criterion :  “friedman_mse” for the mean squared error with improvement score by Friedman, “mse” for mean squared error, and “mae” for the mean absolute error. best is friedman!\n",
    "    # init : ?\n",
    "    # presort : whether to presort data in order to speedup the alg. best is auto!\n",
    "    clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=n_est,\\\n",
    "                                       subsample=1.0, criterion='friedman_mse',\\\n",
    "                                       min_samples_split=2, min_samples_leaf=1,\\\n",
    "                                       min_weight_fraction_leaf=0.0, max_depth=3,\\\n",
    "                                       min_impurity_decrease=0.0, min_impurity_split=None,\\\n",
    "                                       init=None, random_state=None, max_features=None,\\\n",
    "                                       verbose=0, max_leaf_nodes=None, warm_start=False,\\\n",
    "                                       presort='auto')\n",
    "    # Fitting the model\n",
    "    clf.fit(training_images, training_labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the methods\n",
    "Now we want to compare the methods in terms of misclassifications of test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "# misclassification rate of Random Forest Classifier\n",
    "rf_mis_rate = []\n",
    "# misclassification rate of Gradient Boosting Classifier\n",
    "gb_mis_rate = []\n",
    "# between 10 and 1200 trees\n",
    "for n in range(10, 1210,30):\n",
    "    \n",
    "    rf_clf = rfclf(n)\n",
    "    rf_pred = rf_clf.predict(test_images)\n",
    "    rf_mis = [(p_i - l_i == 0) for p_i, l_i in zip(rf_pred, test_labels)]\n",
    "    rf_mis_rate.append(sum(rf_mis) / 3500)\n",
    "    \n",
    "    gb_clf = gbclf(n)\n",
    "    gb_pred = gb_clf.predict(test_images)\n",
    "    gb_mis = [(p_i - l_i == 0) for p_i, l_i in zip(gb_pred, test_labels)]\n",
    "    gb_mis_rate.append(sum(gb_mis) / 3500)\n",
    "    \n",
    "plt.plot(range(10, 1210,30), rf_mis_rate , 'b', range(10, 1210,30), gb_mis_rate, 'r')\n",
    "plt.show()\n",
    "\n",
    "# took more than a hour!!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
