{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data\n",
    "Fetching MINST dataset and plotting the first image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ea3c358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Dataset downloader\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# Working with datastructures such as matrix\n",
    "import numpy as np\n",
    "# Plot in python\n",
    "import matplotlib.pyplot as plt\n",
    "# Dirac delta for misclassification rate\n",
    "from sympy.functions.special.delta_functions import DiracDelta\n",
    "# plot the random forest\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import check_call\n",
    "import pydot\n",
    "import pydot_ng\n",
    "from subprocess import check_call\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fetching MNIST dataset with fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"/Users/elihei/programming/Practical Machine Learning/data/MNIST\")\n",
    "\n",
    "# reshape images to 28 * 28 in order to plot\n",
    "images = [x.reshape(28,28) for x in mnist.data]\n",
    "labels = mnist.target\n",
    "\n",
    "# plotting the first image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0], cmap=plt.cm.gray_r)\n",
    "plt.title('Training: %i' % labels[0])\n",
    "plt.show()\n",
    "plt.close()\n",
    "images = mnist.data\n",
    "\n",
    "# making (label, features) --> each pixel is a feature so we have 784 features for each sample\n",
    "# half of the data is used for training and half will be used for testing.\n",
    "indice = np.random.choice(len(images), len(images)//2, replace=False) # random choice of half of the indices --> to be unbiased!\n",
    "training_images = images[indice]\n",
    "training_labels = labels[indice]\n",
    "test_images = images[[x for x in range(0,len(images)) if x not in indice]]\n",
    "test_labels = labels[[x for x in range(0,len(images)) if x not in indice]]\n",
    "# making classes \n",
    "map(int, training_labels)\n",
    "training_size = len(training_images)\n",
    "\n",
    "# ATTENTION! the data should be in a 2d array format. Each image is a row and\n",
    "# all of the images should compose a 2d array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "RandomForestClassifier function has been used to set the parameteres.\n",
    "- The function \"misclass\" gets the max depth of the trees of the forest and returns the misclassification rate for 10-fold cross-validation.\n",
    "- The funciton opt_param finds the optimal depth of the trees by a grid search. (optimal depth was 190 for a grid serach between 10 and 200 by step of length 10)\n",
    "- The first classifer tree fitted on the training data has been plotted and the plot is saved in tree0.png.\n",
    "- In order to find the best number of classifiers for this problem the out-of-bag error among the number of trees has been plotted. (optimal number of trees was \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/tree/export.py:399: DeprecationWarning: out_file can be set to None starting from 0.18. This will be the default in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "# Fits a random forest to the training data with maximum depth of m_depth and returns \n",
    "# the misclassification rate for 10-fold cross-validation\n",
    "def misclass(m_depth):\n",
    "    # Define Random Forest Classifier by setting the parameters\n",
    "    # n_estimators : number of random trees\n",
    "    # criterion : mesearement of feature importance \n",
    "    # max_depth : maximum depth of trees in the forest\n",
    "    # min_samples_split : minimum number of samples in a split\n",
    "    # min_samples_leaf : minimum number of samples in a leaf\n",
    "    # min_weight_fraction_leaf : minimum weight of samples to be a leaf\n",
    "    # n_jobs : number of cores dedicated\n",
    "    # bootstrap : whether to use bootstrap for sampling or not\n",
    "    # oob_score : whether to calculate out of box score or not\n",
    "    # max_fetures : maximum fetures used for classification at each split\n",
    "    # max_leaf_nodes : maximum number of leaf nodes\n",
    "    # min_impurity_split : if the impurity of a node is more than this the node can be splited.\n",
    "    # min_impurity_decrease : if the impurity of the nodes resulted from spliting a node is more than this the node can be splited.\n",
    "    # warm_start : Whether to use previous model and add new estimators\n",
    "    # random_state : seed of the random generator\n",
    "    # verbose : ?\n",
    "    \n",
    "    # setting the parameters of the classifier\n",
    "    rfclassifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=m_depth, min_samples_split=2, min_samples_leaf=20,\\\n",
    "                           min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\\\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False,\\\n",
    "                           n_jobs=3, random_state=None, verbose=0, warm_start=False)\n",
    "    mis_rate = 0\n",
    "    # the following is a 10-fold cross-validation\n",
    "    for x in range(0,10):\n",
    "        min_ind = x * training_size // 10\n",
    "        max_ind = (x + 1) * training_size // 10\n",
    "        # validation data\n",
    "        vd_img = training_images[min_ind:max_ind]\n",
    "        vd_lab = training_labels[min_ind:]\n",
    "        # training data\n",
    "        tr_img = np.concatenate((training_images[:min_ind] , training_images[max_ind:]), axis = 0)\n",
    "        tr_lab = np.concatenate((training_labels[:min_ind], training_labels[max_ind:]), axis = 0)\n",
    "        # fitting the model to training data\n",
    "        rfclassifier.fit(tr_img, tr_lab)\n",
    "        # predicting the labels of validation data\n",
    "        predicitons = rfclassifier.predict(vd_img)\n",
    "        # computing misclassificaitons of validation data\n",
    "        misclassifications = [(p_i - l_i == 0) for p_i, l_i in zip(predicitons, vd_lab)]\n",
    "        mis_rate += sum(misclassifications) / 3500\n",
    "    return 1- mis_rate / 10\n",
    "\n",
    "# optimizer finder employing grid search between mini and maxi with grid width of step\n",
    "def opt_param(mini, maxi, step):\n",
    "    opt = 1\n",
    "    for x in range(mini, maxi, step):\n",
    "        if opt > misclass(x):\n",
    "            opt = misclass(x)\n",
    "            optx = x\n",
    "    return optx\n",
    "\n",
    "# fits a classifier with n_est trees to the training data\n",
    "def rfclf(n_est):\n",
    "    # A classifier description --> oob_score = True for future use in plotting\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, criterion='gini', max_depth= 190, min_samples_split=2, min_samples_leaf=20,\\\n",
    "                               min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None,\\\n",
    "                               min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=True,\\\n",
    "                               n_jobs=3, random_state=None, verbose=0, warm_start=True)\n",
    "\n",
    "    # Fitting the defined classifier\n",
    "    clf.fit(training_images, training_labels)\n",
    "    return clf\n",
    "    \n",
    "rf_clf = rfclf(100)\n",
    "\n",
    "# plotting the first classifier tree\n",
    "tree = rf_clf.estimators_[0]\n",
    "export_graphviz(tree,\n",
    "            filled=True,\n",
    "            rounded=True)\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree0.png')\n",
    "\n",
    "# plotting out-of-bag error for different number of trees\n",
    "oob_errors = []\n",
    "for n in range(10, 1210,30):\n",
    "    rf_clf = rfclf(n)\n",
    "    oob_errors.append(1 - rf_clf.oob_score_)   \n",
    "plt.plot(range(10, 1210,30), oob_errors, 'r')\n",
    "plt.show()  \n",
    "\n",
    "# Took about 30 mins on core i5 intel --> best number of trees =? 500 (best performance without overhead)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits a classifier with n_est trees to the training data\n",
    "def gbclf(n_est):\n",
    "    # Defining a Gradient Boosting Classifier (many of parameters are the same as of RF classifier)\n",
    "    # loss : Gradient Boosting (deviance) or Adaboost (exponential)\n",
    "    # learning rate : rate of decrease in weight of residual fitted tree\n",
    "    # subsample : fraction of data to be used if < 1 --> stochastic gradient descent\n",
    "    # criterion :  “friedman_mse” for the mean squared error with improvement score by Friedman, “mse” for mean squared error, and “mae” for the mean absolute error. best is friedman!\n",
    "    # init : ?\n",
    "    # presort : whether to presort data in order to speedup the alg. best is auto!\n",
    "    clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=n_est,\\\n",
    "                                       subsample=1.0, criterion='friedman_mse',\\\n",
    "                                       min_samples_split=2, min_samples_leaf=1,\\\n",
    "                                       min_weight_fraction_leaf=0.0, max_depth=3,\\\n",
    "                                       min_impurity_decrease=0.0, min_impurity_split=None,\\\n",
    "                                       init=None, random_state=None, max_features=None,\\\n",
    "                                       verbose=0, max_leaf_nodes=None, warm_start=False,\\\n",
    "                                       presort='auto')\n",
    "    # Fitting the model\n",
    "    clf.fit(training_images, training_labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the methods\n",
    "Now we want to compare the methods in terms of misclassifications of test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mis_rate = []\n",
    "gb_mis_rate = []\n",
    "for n in range(10, 1210,30):\n",
    "    rf_clf = rfclf(n)\n",
    "    rf_pred = rf_clf.predict(test_images)\n",
    "    rf_mis = [(p_i - l_i == 0) for p_i, l_i in zip(rf_pred, test_labels)]\n",
    "    rf_mis_rate.append(sum(rf_mis) / 3500)\n",
    "    gb_clf = gbclf(n)\n",
    "    gb_pred = gb_clf.predict(test_images)\n",
    "    gb_mis = [(p_i - l_i == 0) for p_i, l_i in zip(gb_pred, test_labels)]\n",
    "    gb_mis_rate.append(sum(gb_mis) / 3500)\n",
    "plt.plot(range(10, 1210,30), rf_mis_rate , 'b', range(10, 1210,30), gb_mis_rate, 'r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
